from metrics import BaseMetric, DataProvider

class DTCR(BaseMetric):
    """ This function computes for each point the distance to the clostest record in the trainings data set.
        Returns a list with all the minimal distances. 
        
        Computes absolute distance for the numerical values and 1 for change in categorical values.
    """
    
    def __init__(self, dp: DataProvider, n_samples = 5, euclidean=False):
        """ Initialize the DCR metric.
            DP the data provider object.
            n_samples: Number of test points to compute the metric for.
        """
        super().__init__(dp)
        self.n_samples = n_samples
        self.euclidean = euclidean
        self.original_data = dict()

    def __call__(self, dataset_name: str, model_name: str, train_synthetic=True):
        """ Compute the metric. 
            :param dataset_name: Name of the dataset, used to obtain dataset meta-info from the config file.
            :param model_name: Name of the model to use from the DataProvider
            :param train_synthetic: Use samples from the synthetic train or test set (as they are generated by the same function, 
                    this should not make a difference)
        """
        self.num_cols = self.dp.get_config(dataset_name)["num_cols"]
        self.cat_cols = self.dp.get_config(dataset_name)["cat_cols"]
        
        # Load synthetic or test dataset
        if model_name == "original":
            df_gen = self.dp.get_random_data_sample("original", n_samples=self.n_samples, train=False)
        else:
            df_gen = self.dp.get_random_data_sample(model_name, n_samples=self.n_samples, train=train_synthetic)
            
        print(df_gen.shape)
        
        # Load original train dataset
        if dataset_name not in self.original_data:
            self.original_data[dataset_name] =  self.dp.get_full_data("original")
            
        # Compute the DTCR
        if self.euclidean:
            mins = self.check_similarity_euclidean(self.original_data[dataset_name], df_gen)
        else:
            mins = self.check_similarity(self.original_data[dataset_name], df_gen)
                              
        return {"mins": mins}        
    
    # Use L1 norm
    def check_similarity(self, df_orig, df_gen):
        mins = []

        for i in range(df_gen.shape[0]):
            sample = df_gen.iloc[i]
            
            min_num = abs(df_orig[self.num_cols] - sample[self.num_cols]).sum(axis=1)
            min_cat = (~(df_orig[self.cat_cols] == sample[self.cat_cols])).sum(axis=1)
            min_total = (min_num + min_cat).min()
            mins.append(min_total)

        return mins
    
    # Use L2 norm
    def check_similarity_euclidean(self, df_orig, df_gen):
        mins = []

        for i in range(df_gen.shape[0]):
            sample = df_gen.iloc[i]
            
            min_num = np.sqrt(((df_orig[self.num_cols] - sample[self.num_cols])**2).sum(axis=1))
            min_cat = (~(df_orig[self.cat_cols] == sample[self.cat_cols])).sum(axis=1)
            min_total = (min_num + min_cat).min()
            mins.append(min_total)
            
        return mins

